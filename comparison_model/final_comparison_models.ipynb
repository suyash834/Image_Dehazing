{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0276fe-824a-4b4b-a844-6e73dc2958ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '4_layers_model/best_model_4_4.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# === Load Models ===\u001b[39;00m\n\u001b[32m    128\u001b[39m models = {\n\u001b[32m    129\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m4\u001b[39m\u001b[33m\"\u001b[39m: DeepDehazeNet(\u001b[32m4\u001b[39m).to(device),\n\u001b[32m    130\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m8\u001b[39m\u001b[33m\"\u001b[39m: DeepDehazeNet(\u001b[32m8\u001b[39m).to(device),\n\u001b[32m    131\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m16\u001b[39m\u001b[33m\"\u001b[39m: DeepDehazeNet(\u001b[32m16\u001b[39m).to(device),\n\u001b[32m    132\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m models[\u001b[33m\"\u001b[39m\u001b[33m4\u001b[39m\u001b[33m\"\u001b[39m].load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4_layers_model/best_model_4_4.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    134\u001b[39m models[\u001b[33m\"\u001b[39m\u001b[33m8\u001b[39m\u001b[33m\"\u001b[39m].load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33m8_layers_model/best_model_8_8.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=device))\n\u001b[32m    135\u001b[39m models[\u001b[33m\"\u001b[39m\u001b[33m16\u001b[39m\u001b[33m\"\u001b[39m].load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33m16_layers_model/best_model_16_16.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=device))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env1\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env1\\Lib\\site-packages\\torch\\serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\env1\\Lib\\site-packages\\torch\\serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '4_layers_model/best_model_4_4.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# === Model Definition ===\n",
    "class DeepDehazeNet(nn.Module):\n",
    "    def __init__(self, layers=8):\n",
    "        super().__init__()\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            )\n",
    "        self.layers = layers\n",
    "        if layers == 4:\n",
    "            self.enc1 = conv_block(3, 64)\n",
    "            self.pool1 = nn.MaxPool2d(2)\n",
    "            self.bottleneck = conv_block(64, 128)\n",
    "            self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "            self.dec1 = conv_block(128, 64)\n",
    "            self.final = nn.Conv2d(64, 3, 1)\n",
    "        elif layers == 8:\n",
    "            self.enc1 = conv_block(3, 64)\n",
    "            self.pool1 = nn.MaxPool2d(2)\n",
    "            self.enc2 = conv_block(64, 128)\n",
    "            self.pool2 = nn.MaxPool2d(2)\n",
    "            self.enc3 = conv_block(128, 256)\n",
    "            self.pool3 = nn.MaxPool2d(2)\n",
    "            self.bottleneck = conv_block(256, 512)\n",
    "            self.up1 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "            self.dec1 = conv_block(512, 256)\n",
    "            self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "            self.dec2 = conv_block(256, 128)\n",
    "            self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "            self.dec3 = conv_block(128, 64)\n",
    "            self.final = nn.Conv2d(64, 3, 1)\n",
    "        elif layers == 16:\n",
    "            self.enc1 = conv_block(3, 64)\n",
    "            self.pool1 = nn.MaxPool2d(2)\n",
    "            self.enc2 = conv_block(64, 128)\n",
    "            self.pool2 = nn.MaxPool2d(2)\n",
    "            self.enc3 = conv_block(128, 256)\n",
    "            self.pool3 = nn.MaxPool2d(2)\n",
    "            self.enc4 = conv_block(256, 512)\n",
    "            self.pool4 = nn.MaxPool2d(2)\n",
    "            self.enc5 = conv_block(512, 1024)\n",
    "            self.pool5 = nn.MaxPool2d(2)\n",
    "            self.bottleneck = conv_block(1024, 2048)\n",
    "            self.up1 = nn.ConvTranspose2d(2048, 1024, 2, stride=2)\n",
    "            self.dec1 = conv_block(2048, 1024)\n",
    "            self.up2 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "            self.dec2 = conv_block(1024, 512)\n",
    "            self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "            self.dec3 = conv_block(512, 256)\n",
    "            self.up4 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "            self.dec4 = conv_block(256, 128)\n",
    "            self.up5 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "            self.dec5 = conv_block(128, 64)\n",
    "            self.final = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layers == 4:\n",
    "            e1 = self.enc1(x)\n",
    "            b = self.bottleneck(self.pool1(e1))\n",
    "            d1 = self.dec1(torch.cat([self.up1(b), e1], 1))\n",
    "            return torch.sigmoid(self.final(d1))\n",
    "        elif self.layers == 8:\n",
    "            e1 = self.enc1(x)\n",
    "            e2 = self.enc2(self.pool1(e1))\n",
    "            e3 = self.enc3(self.pool2(e2))\n",
    "            b = self.bottleneck(self.pool3(e3))\n",
    "            d1 = self.dec1(torch.cat([self.up1(b), e3], 1))\n",
    "            d2 = self.dec2(torch.cat([self.up2(d1), e2], 1))\n",
    "            d3 = self.dec3(torch.cat([self.up3(d2), e1], 1))\n",
    "            return torch.sigmoid(self.final(d3))\n",
    "        elif self.layers == 16:\n",
    "            e1 = self.enc1(x)\n",
    "            e2 = self.enc2(self.pool1(e1))\n",
    "            e3 = self.enc3(self.pool2(e2))\n",
    "            e4 = self.enc4(self.pool3(e3))\n",
    "            e5 = self.enc5(self.pool4(e4))\n",
    "            b = self.bottleneck(self.pool5(e5))\n",
    "            d1 = self.dec1(torch.cat([self.up1(b), e5], 1))\n",
    "            d2 = self.dec2(torch.cat([self.up2(d1), e4], 1))\n",
    "            d3 = self.dec3(torch.cat([self.up3(d2), e3], 1))\n",
    "            d4 = self.dec4(torch.cat([self.up4(d3), e2], 1))\n",
    "            d5 = self.dec5(torch.cat([self.up5(d4), e1], 1))\n",
    "            return torch.sigmoid(self.final(d5))\n",
    "\n",
    "# === Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === Paths ===\n",
    "hazy_dir = \"hazy\"\n",
    "clean_dir = \"clear\"\n",
    "results_dir = \"results_final\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# === Load image paths and randomly sample 10 pairs ===\n",
    "all_hazy_paths = sorted(glob.glob(os.path.join(hazy_dir, \"*\")))\n",
    "all_clean_paths = sorted(glob.glob(os.path.join(clean_dir, \"*\")))\n",
    "\n",
    "paired_paths = list(zip(all_hazy_paths, all_clean_paths))\n",
    "random.shuffle(paired_paths)\n",
    "selected_pairs = paired_paths[:30]\n",
    "\n",
    "hazy_imgs = [transform(Image.open(p[0]).convert(\"RGB\")) for p in selected_pairs]\n",
    "clean_imgs = [transform(Image.open(p[1]).convert(\"RGB\")) for p in selected_pairs]\n",
    "\n",
    "# === Load Models ===\n",
    "models = {\n",
    "    \"4\": DeepDehazeNet(4).to(device),\n",
    "    \"8\": DeepDehazeNet(8).to(device),\n",
    "    \"16\": DeepDehazeNet(16).to(device),\n",
    "}\n",
    "models[\"4\"].load_state_dict(torch.load(\"4_layers_model/best_model_4_4.pth\", map_location=device))\n",
    "models[\"8\"].load_state_dict(torch.load(\"8_layers_model/best_model_8_8.pth\", map_location=device))\n",
    "models[\"16\"].load_state_dict(torch.load(\"16_layers_model/best_model_16_16.pth\", map_location=device))\n",
    "for m in models.values():\n",
    "    m.eval()\n",
    "\n",
    "# === Inference and Evaluation ===\n",
    "rows = []\n",
    "psnr_dict, ssim_dict, time_dict = {\"4\": [], \"8\": [], \"16\": []}, {\"4\": [], \"8\": [], \"16\": []}, {\"4\": [], \"8\": [], \"16\": []}\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "for i in range(30):\n",
    "    hazy = hazy_imgs[i].unsqueeze(0).to(device)\n",
    "    clean = clean_imgs[i].unsqueeze(0).to(device)\n",
    "\n",
    "    row = [hazy_imgs[i], clean_imgs[i]]\n",
    "    for k in [\"4\", \"8\", \"16\"]:\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = models[k](hazy)\n",
    "        elapsed = time.time() - start\n",
    "        output_img = output.squeeze().cpu()\n",
    "        row.append(output_img)\n",
    "\n",
    "        psnr_val = compare_psnr(clean.squeeze().cpu().permute(1, 2, 0).numpy(),\n",
    "                                output_img.permute(1, 2, 0).numpy(), data_range=1.0)\n",
    "        ssim_val = compare_ssim(clean.squeeze().cpu().permute(1, 2, 0).numpy(),\n",
    "                                output_img.permute(1, 2, 0).numpy(), data_range=1.0, channel_axis=-1)\n",
    "\n",
    "        psnr_dict[k].append(psnr_val)\n",
    "        ssim_dict[k].append(ssim_val)\n",
    "        time_dict[k].append(elapsed)\n",
    "\n",
    "        # Create label image\n",
    "        label_img = Image.new(\"RGB\", (256, 256), (0, 0, 0))\n",
    "        d = ImageDraw.Draw(label_img)\n",
    "        d.text((10, 50), f\"PSNR: {psnr_val:.2f}\", fill=\"white\", font=font)\n",
    "        d.text((10, 100), f\"SSIM: {ssim_val:.3f}\", fill=\"white\", font=font)\n",
    "        d.text((10, 150), f\"Time: {elapsed:.4f}s\", fill=\"white\", font=font)\n",
    "        row.append(transforms.ToTensor()(label_img))\n",
    "\n",
    "    rows.append(torch.stack(row))\n",
    "\n",
    "# === Create Grid Image ===\n",
    "grid = torch.cat(rows, dim=0)\n",
    "grid_img = vutils.make_grid(grid, nrow=8, padding=2)\n",
    "grid_img_np = (grid_img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "img = Image.fromarray(grid_img_np)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Add column labels\n",
    "columns = [\"Hazy\", \"GT\", \"4L Output\", \"4L Metrics\", \"8L Output\", \"8L Metrics\", \"16L Output\", \"16L Metrics\"]\n",
    "for idx, label in enumerate(columns):\n",
    "    draw.text((idx * 256 + 10, 5), label, fill=\"white\", font=font)\n",
    "\n",
    "img.save(os.path.join(results_dir, \"comparison_table_full.png\"))\n",
    "print(\"✅ Saved full comparison table with labels and metrics.\")\n",
    "\n",
    "# === PSNR & SSIM Line Plots ===\n",
    "x = list(range(1, 31))\n",
    "for metric, name, dictionary in [(psnr_dict, \"PSNR\", psnr_dict), (ssim_dict, \"SSIM\", ssim_dict)]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for k in [\"4\", \"8\", \"16\"]:\n",
    "        plt.plot(x, dictionary[k], label=f\"{k} Layers\")\n",
    "    plt.xlabel(\"Image Index\")\n",
    "    plt.ylabel(name)\n",
    "    plt.title(f\"{name} vs Image Index\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(results_dir, f\"{name.lower()}_vs_index.png\"))\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved {name} vs index plot.\")\n",
    "\n",
    "# === Inference Time Bar Graph (Per Image) ===\n",
    "bar_width = 0.25\n",
    "x_indexes = np.arange(30)\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i, k in enumerate([\"4\", \"8\", \"16\"]):\n",
    "    plt.bar(x_indexes + i * bar_width, time_dict[k], width=bar_width, label=f\"{k} Layers\")\n",
    "plt.xlabel(\"Image Index\")\n",
    "plt.ylabel(\"Inference Time (s)\")\n",
    "plt.title(\"Inference Time per Image\")\n",
    "plt.xticks(x_indexes + bar_width, [str(i+1) for i in range(30)])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(results_dir, \"inference_time_per_image.png\"))\n",
    "plt.close()\n",
    "print(\"✅ Saved inference time bar graph per image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b4c534-48e4-4a24-8c35-6935d8520f4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Average Inference Time Bar Graph (Model-wise) ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m avg_times = {k: \u001b[43mnp\u001b[49m.mean(time_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33m4\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m8\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m16\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m      4\u001b[39m models_list = [\u001b[33m\"\u001b[39m\u001b[33m4 Layers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m8 Layers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m16 Layers\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m avg_time_values = [avg_times[\u001b[33m\"\u001b[39m\u001b[33m4\u001b[39m\u001b[33m\"\u001b[39m], avg_times[\u001b[33m\"\u001b[39m\u001b[33m8\u001b[39m\u001b[33m\"\u001b[39m], avg_times[\u001b[33m\"\u001b[39m\u001b[33m16\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# === Average Inference Time Bar Graph (Model-wise) ===\n",
    "avg_times = {k: np.mean(time_dict[k]) for k in [\"4\", \"8\", \"16\"]}\n",
    "\n",
    "models_list = [\"4 Layers\", \"8 Layers\", \"16 Layers\"]\n",
    "avg_time_values = [avg_times[\"4\"], avg_times[\"8\"], avg_times[\"16\"]]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models_list, avg_time_values, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Average Inference Time (s)\")\n",
    "plt.title(\"Average Inference Time per Model\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add the average time value on top of each bar\n",
    "for bar, avg_time in zip(bars, avg_time_values):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.0005, f\"{avg_time:.4f}s\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"average_inference_time_per_model.png\"))\n",
    "plt.close()\n",
    "print(\"✅ Saved average inference time bar graph per model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcf969-b1ef-449e-8b24-943ceead0801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
